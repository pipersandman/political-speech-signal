name: build-and-deploy

on:
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      TRUTH_REPO: https://github.com/stiles/trump-truth-social-archive.git
      CACHE_BUST: ${{ github.sha }}
      SNAP_MAX: "200"
      NLTK_DATA: ${{ runner.temp }}/nltk_data
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Prepare NLTK dir
        run: mkdir -p "$NLTK_DATA"

      - name: Cache NLTK data
        uses: actions/cache@v4
        with:
          path: ${{ env.NLTK_DATA }}
          key: nltk-${{ runner.os }}-v1

      - name: Install deps
        run: |
          pip install "pandas>=2.2" "pyarrow>=16" "duckdb>=1.0" "nltk>=3.9"
          python - << 'PY'
          import os, nltk
          nltk.data.path.append(os.environ['NLTK_DATA'])
          nltk.download('vader_lexicon', download_dir=os.environ['NLTK_DATA'])
          nltk.download('stopwords',     download_dir=os.environ['NLTK_DATA'])
          PY

      - name: Normalize → snapshot.json
        run: python scripts/normalize_truth.py

      - name: Normalize to Parquet (+ trends)
        run: python scripts/normalize_to_parquet.py

      - name: NLP analysis → nlp.json
        run: python scripts/nlp_analyze.py

      - name: Build site (inventory + snapshot + NLP)
        run: python scripts/inventory_truth.py

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
